{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52e64fe",
   "metadata": {},
   "source": [
    "# Подготовка и загрузка данных для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09659194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_dir = \"leapGestRecog\"\n",
    "\n",
    "# Подготовка данных\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Загрузка и подготовка данных\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # По умолчанию стоит 'categorical', если у нас несколько классов\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # По умолчанию стоит 'categorical', если у нас несколько классов\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095d34f",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3063fb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 220s 433ms/step - loss: 0.3439 - accuracy: 0.9350 - val_loss: 1.3034 - val_accuracy: 0.5675\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 204s 409ms/step - loss: 0.0502 - accuracy: 0.9879 - val_loss: 1.3090 - val_accuracy: 0.5878\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 197s 394ms/step - loss: 0.0326 - accuracy: 0.9890 - val_loss: 1.3394 - val_accuracy: 0.5652\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 196s 392ms/step - loss: 0.0261 - accuracy: 0.9896 - val_loss: 1.4394 - val_accuracy: 0.5548\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 196s 392ms/step - loss: 0.0229 - accuracy: 0.9904 - val_loss: 1.4638 - val_accuracy: 0.5545\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 198s 396ms/step - loss: 0.0206 - accuracy: 0.9910 - val_loss: 1.4427 - val_accuracy: 0.5663\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 202s 403ms/step - loss: 0.0201 - accuracy: 0.9904 - val_loss: 1.4762 - val_accuracy: 0.5645\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 234s 467ms/step - loss: 0.0196 - accuracy: 0.9900 - val_loss: 1.5050 - val_accuracy: 0.5907\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 211s 422ms/step - loss: 0.0188 - accuracy: 0.9910 - val_loss: 1.5548 - val_accuracy: 0.5698\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 208s 416ms/step - loss: 0.0171 - accuracy: 0.9904 - val_loss: 1.5797 - val_accuracy: 0.5620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19893d55610>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Создаем модель\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(10, activation='softmax')  # Указываем количество классов (10 в данном случае)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Подготовка данных\n",
    "train_generator = train_generator  # Загрузите и подготовьте данные здесь\n",
    "\n",
    "# Обучение модели\n",
    "model.fit(train_generator, epochs=10, validation_data=validation_generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965e6d4",
   "metadata": {},
   "source": [
    "# Сохраняем модель для внедение в приложения и пишем само приложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519704b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"hand_gesture_model.h5\")  # Сохраняем модель в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426207e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f0a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка предварительно обученной модели\n",
    "model = load_model(\"hand_gesture_model.h5\")  # Замените на фактический путь к вашей модели\n",
    "\n",
    "# Захват видеопотока с веб-камеры\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Инициализация детектора лиц Dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Детекция лиц\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    # Если лицо обнаружено, обработка изображения\n",
    "    if len(faces) > 0:\n",
    "        face_detected = True\n",
    "        for face in faces:\n",
    "            x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "            face_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "            # Обработка изображения (например, изменение размера, нормализация)\n",
    "            processed_frame = cv2.resize(face_roi, (224, 224))  # Пример изменения размера\n",
    "\n",
    "            # Предсказание жестов рук с использованием модели\n",
    "            predictions = model.predict(np.expand_dims(processed_frame, axis=0))\n",
    "\n",
    "            # Определение жеста с максимальной вероятностью\n",
    "            gesture_class = np.argmax(predictions)\n",
    "\n",
    "            # Вывод результата на экран\n",
    "            cv2.putText(frame, f\"Gesture: {gesture_class}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Вывод кадра на экран\n",
    "    cv2.imshow('Hand Gesture Recognition', frame)\n",
    "\n",
    "    # Выход из цикла по нажатию клавиши 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Освобождение ресурсов\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
